# 포트폴리오에 사용한 데이터 정리

## 1차 프로젝트 : 공공자전거 이용현황분석

서울시 공공데이터 포털에서 제공하는 공공자전거 이용현황 CSV파일을 다운로드받아 Pandas, Numpy등으로 분석 및 Django를 이용하여 시각화 및 로컬 배포

## 2차 프로젝트 : 3세대 골목상권입주를 위한 데이터셋 구축

서울시 공공데이터 포털에서 제공하는 생활 인구 데이터 / 평당 상가 가격 데이터 / 상가밀도 데이터 / 지하철,버스 데이터를 각각 API, CSV를 이용하여 수집 및 정제를 진행

골목상권에 기여하는 로컬 크리에이터 상점 정보를 수집하기위해 MZ세대들이 가장 많이 이용하는 '아이디어스'를 크롤링하여 로컬 크리에이터의 상점 정보 수집. 이후 상점들의 정확한 위치 및 정보, 그리고 신뢰성있는 가게들만을 골라내기위해 SPARK로 전처리 진행.

**데이터 크기 및 사용 데이터**

- 사용데이터:총 12개의 데이터 사용 , 총 ROW 1,377,128개 사용

- 결과 데이터셋 : ROW 1100개의 유의미한 데이터를 얻을수있었다.

다만, 프로젝트 인원의 갑작스러운 돌발행동 및 이탈로 모든 데이터를 통합하진못하고 어느정도 신뢰성있는 데이터만 병합하여 병합한 데이터셋을 토대로 Django로 시각화 진행.


## 3차 프로젝트 : 반려동물 안구질환 판별 서비스

융합프로젝트로, DE + DA 함께 진행한 웹서비스. 현재 서비스 배포중이며 GCP를 이용하여 서버를 구축하였다.

실시간 트위터 토픽 수집을 위한 Kafka와 zookeeper는 로컬에서 노트북1대 대여 + 기존 노트북 1대 와 로컬 PC를 엮어 도커로 구현하였으나,
트위터의 권한 및 서버문제로 제대로 기능구현까지 이루어지지않았다. 또한 현재는 장비의 반납 및 고장때문에 부득이하게 GCP로 로컬에서 구현했던것처럼 서버를 구성해놨지만, 다른 서버들 또한 GCP로 구성되어있기때문에 따로 도커는 사용하지않고 재구현하였다.

데이터 파이프라인으로 airflow를 통해 데일리로 뉴스기사를 수집 및 정제, 적재를 처리하였으며 그렇게 적재된 데이터들을 이용하여 Django로 웹 서비스를 구현 및 보안 배포를 진행함.


**데이터 크기**

- Airflow를통해 크롤링된 뉴스기사 : 500 CSV

- 행정안전부 제공 전국 동물병원 리스트 : 9590 CSV

- 뉴스기사 문서 요약 AI 학습용 텍스트 : 기사원문 40만 / 요약문 80만 JSON

- 반려동물 안구 질환 학습용 이미지 데이터 : 초기 8600 / 비성숙 : 8600 / 성숙 : 8600 / 증상없음 : 8600  PNG/JPG

총 30963장의 이미지로 학습.

**AI모델**

- 이미지 분류 AI모델은, 사전학습 모델중 <u> EffcientNetB4모델이 성능이 가장 좋게 </u>나와서 해당 모델 채택.

- 텍스트 요약 AI모델은 KoBART와 구글의 TextRank를 사용, 학습결과 TextRank 모델이 KoBART 모델보다 학습시간이 적고 성능이 우수하게 나왔으므로 뉴스기사 요약에 <u>TextRank</u>모델을 적용시켰다.

**파이프라인 설명**

Airflow에서 메세지 브로커로 RabbitMQ 대신 Redis를 사용한 이유

    airflow로만든 데이터 수집 및 정제, 적재 파이프라인은 뉴스기사 특성상 어느정도 데이터의 유실이 있어도 괜찮은 데이터이며, 자주 수집할것이기때문에 속도가 더 중요하다고 판단하였기때문에 Redis를 채택.


