# 프로젝트를 진행하면서 사용한 기술스택들에대한 기본 개념정리

많지는않지만 프로젝트들을 진행하면서 여러 기술들을 사용하게되었다.
보통 프로젝트에 기술을 적용하면서 바로바로 기본개념들을 간단하게 정리해놨지만 기술들의 갯수가 많아지고 프로젝트에 적용시키면서 트러블슈팅들을 같이 작성해놓으니까 찾아보기가 힘들어지는 문제점이생겼다.

그래서 프로젝트에 이용했던 기술 스택들에대한 기본 개념들을 이곳에 정리해놓고 필요할때 찾아보는편이 낫겠다고 판단하여 작성.

- Celery
- Redis
- Airflow
- Django
- GCP
- Kafka
- Hadoop & hdfs
- Spark
- Elastic search
- logstash
- Kibana 
- SQL
- Nginx
- uWsgi
- Zookeeper
- Pandas
- Numpy
- Docker
- CloudFlare
## Celery와 사용이유

Celery는 많은 작업을 분산처리를 할 수있도록 해주는 Task queue이다. <u>Task는 비동기 처리를 도와주는 친구로 Celery는 비동기 작업 큐 라고 생각하면된다.</u>

<u>그럼 비동기 작업이 왜 필요한가?</u> 

이건 간단하게 생각하면 쉽다.
예를 들어 내가 스타벅스의 매장을 조회하기위하여 스타벅스 홈페이지에 접속하여 매장 조회를 눌렀다고 가정하자.

그런데 웹 페이지의 경우, 전국의 스타벅스 매장의 정보를 DB에서 가져오기때문에 이 가져오는 작업이 끝날때까지 나는 하염없이 웹 페이지의 응답을 기다려야한다.

그래서 그러한 무거운 연산 및 처리들을 Task라는 비동기 작업을 처리해주는 큐에 넘겨주고, 이용자는 다른작업을 할 수 있도록하는것이 원리이다.
그런고로 Celery는 클라이언트와 worker사이를 중재하는 메세지 브로커를 사용하여 통신하게되는데, worker들에게 task를 처리하도록 시키기위해 메세지 브로커로써 Redis나 Rabbit MQ를 사용하게된다.


그럼 queue는 뭘까? 이건 컨테이너벨트에 적재된 화물을 생각하면 쉽다.
예를들어, 택배사에서 컨테이너 벨트위에 배송되야할 택배들이 줄지어있다고치면 이 화물들이 처리해야할 작업이되는것이고 큐는 컨테이너벨트가되는것이다.

그러면 컨테이너벨트에 택배(화물)을 올려주는 작업을해주는 친구가 필요한데, 그 역할을하는게 시스템 브로커이다.

클라이언트에서 처리해야할 작업(task)를 생성하면 celery는 브로커를 통해 worker들에게 메세지를 전달하여 worker들이 해당 작업을 처리하게한다.

celery는 1분에 수백만건의 task를 처리할수있는 속도를 제공하기때문에 분산작업 처리를할때 유용하다.

보통 Airflow+celery+redis or rabbit mq를 많이 사용한다.

## Redis

<u>Airflow와 Redis</u>

redis를 airflow와 함께 사용할때는 보통 메세지 브로커로써 활용한다.
하지만 Redis의 태생 자체가 메세지 브로커가아니라 인 메모리 db로, <u>Key-value형식을가진 Nosql에 속하는 자료구조 서버이다</u>

우리가 흔히 사용하는 DB는 데이터를 물리 디스크에 직접써서 요청,쿼리가 발생할때마다 디스크에 접근하여 데이터를가져온다.
이러한 방법은 <u>서버에 장애가 발생하여도 데이터가 손실되지않는다.</u>

하지만 단점이있다.
사용자가 늘어가서 데이터베이스에 요청되는 쿼리가 커지고 많아진다면 디스크에 접근하는 횟수도 많아지기때문에 DB에 부하가발생하고 속도가 현저히 느려진다.

이때 캐시(메모리 기반) 서버를 도입하게되는데 이 Redis라는것이 인 메모리 DB이다.

캐시서버의 특징은 한번읽어온 데이터를 임의의 공간에 저장하여 다음에 읽을때 새로 읽어오는게 아니라 저장하고있던 정보를 빠르게 받아볼수있게 도와준다.

캐시서버가 한번 읽어온 데이터를 임의의 공간에 저장하고있기때문에, 당연히 매번 데이터베이스를 거치지않기때문에 DB의 부하를 줄이고 서비스 속도도 느려지지않는 장점이있다.

Airflow에서 redis를 같이쓰게되면 redis가 메세지 브로커로써 클라이언트가 생성한 Task가 지정된 큐에따라 redis서버안에 Key-Value타입으로 쌓이는것을 확인할수있다.

만약 내가 airflow task에 큐를 worker-1이라고 지정해놓으면 celery worker를 실행시킬때 워커가 모니터링할 큐를 worker-1이라고 지정해놓으면 해당 worker가 redis 큐 안의 task를 순차적으로 처리하게된다. 

redis 큐 안에 적재되는 task는 <u>task id</u>라는 고유한 task 아이디값이 있어서 아이디 값에따라 순차적으로 처리되는것을 확인할수있었다.

그렇다면 기본 Redis에 대해서 조금 더 알아보자.

Redis의 Key값의 자료형은 String형이지만 Value값의 자료형은 다양한 타입을 지원한다.

서버가 1개라면 굳이 Redis를 사용할필요는없겠지만(인 메모리 데이터베이스로써), 분산환경에서라면 원격 프로세스간의 데이터를 일치시키기가 Redis가 더욱 편리하다.

Redis의 단점은 무엇일까?

1.메모리 기반 DB이기때문에 서버에 장애가 발생했을시 데이터의 유실이있다.

2.메모리 기반 DB이기떄문에 메모리관리가중요하다.

3.싱글 스레드의 특성상, 처리하는데 시간이 오래걸리는 요청이나 명령은 비효율적이다.

### Redis VS Rabbit MQ

둘다 Airflow에서 메세지 브로커로 주로 활용된다.

두가지 다 Airflow와 사용하기 좋지만, 조금씩 특징이 다르다.

<u>Redis</u>

Key-Value를 이용해 Celery가 처리할 작업을 Celery에 보낸 후 캐시에서 해당 Key를 제거하는 방식으로 작동한다.

1.Redis는 데이터 검색을 위해 Database에 접근하기 전 메모리에서 Cache를 가져다 쓴다는 점에서 속도가 빠르다.

2.매우 빠른 서비스 및 메모리 내 기능을 제공하기 때문에 지속성이 중요하지 않고 약간의 손실을 견딜 수있는 짧은 보존 메시지에 적합하다.

3.큰 메시지를 처리 할 때는 대기 시간이 오래 걸린다.


<u> Rabbit MQ</u>

응용 프로그램(applications)에게 메시지를 주고 받을 수 있으며, 메시지가 수신될 때까지 안전하게 있을 수 있도록 하는 공용 플래폼(common platform)을 제공한다.

1.메시지를 다른 대기열로 보낼 수있는 라우팅 시스템을 갖추고 있다.

2.우선 순위가 높은 메시지를 먼저 사용하기 위해 작업자가 사용할 수있는 메시지의 우선 순위를 지원한다.

3.메시지 브로커로서 Redis와 비교할 때 훨씬 더 다양한 기능을 제공한다.
크고 복잡한 메시지에 적합하다.


<mark>나는 뉴스기사를 짧은간격으로 자주 수집해오는것을 목표</mark>로 하였기때문에, 약간의 데이터손실을 감당하고 빠른 속도가 장점인 Redis를 airflow에 적용시켰다.

## AirFlow 

Airflow는 오픈소스 워크플로 관리 플랫폼이다. 아파치 재단에서 만들었고, 파이썬으로 작성되어있으며 워크플로우를 작성할때 파이썬 스크립트를 통해 만들어진다.

Airflow를 접하다보면 <u>워크플로우</u>, <u>DAG</u>,<u>스케줄,스케줄링</u> 라는 단어를 많이 접하게된다. 이것들은 Airflow를 구성하는 아주 기본적인것들이며 Airflow를 사용하게된다면 좋든 싫든 친해져야할 친구들이다.


그럼 Airflow에대해서 상세하게 알아보자.

Airflow는 워크플로우 스케줄링(주기적으로 자동화시키길 원하는 스케줄의 관리)와 감시(모니터링)를하는 기능을 제공한다.

Airflow의 이름을 떠올려보면 이 도구가 뭐하는 도구인지 잘 알수있다.
바람개비인데 바람개비가 바람이불면 순차적으로 돌아가는것처럼 우리가 자동으로 작업되길 원하는 스케줄을 파이썬 스크립트파일을 통해 워크플로우( 의존성으로 연결된 작업들의 집합.)를 작성하면 Airflow가 이 워크플로우를 알아서 관리하고 실행시키고 모니터링한다.

Airflow가하는 역할을 예를들어본다면 이런거다.

우리는 알아서 식사시간이되면 식사를한다.
우리는 사람이기떄문에 알아서 때가되면 요리를하고 음식을만들고 음식을먹는 행동을한다.

**하지만 컴퓨터는 다르다.** 이 친구는 똑똑한것같으면서도 바보이기때문에 우리가 밥을먹으라고 명령을내려야 밥을먹는다.
하지만 이친구는 밥을먹으라고하면 정말 밥만먹는다. 먹을 밥이없으면 밥을만드는 그 전 단계를 처리하지않고 되려 오류를 발생시킨다.

*예를들면, 에러:먹을밥이없습니다. 이런식으로 말이다.*

그러면 우리(사용자)는 컴퓨터에게 하나하나 모든 작업을 스크립트파일로 명령을 내려줘야한다. 

요리 > 밥먹기 

이렇게보면 두개밖에없으니까 괜찮을것같지만 이런일을 매일매일, 식사뿐만아니라 우리가 하루를 보내듯 많은일을 시키려면 굉장히 번거로운 작업이될것이다.

그런것을 한번의 명령으로 컴퓨터가 알아서 동작하게하는것을 도와주는게, **Airflow**다.

단어를 조금 자세하게 살펴보자.

#### 개념 및 단어

- 워크플로우 : 의존성으로 연결된 작업들의 집합. 마트에가서 식자재를 사서와야 요리를할수있고, 요리를해야 점심식사를 할수있다. 이러한 작업들의 집합을 의미한다.

- DAG : 방향이 있는 순환되지않는 그래프(Directed Acyclic Graph)의 약자로 DAG라고 말한다. Task들의 집합이고, 작업집합의 그래프이기때문에 파이프라인으로 볼수있다. 점심식사를하기까지의 전체적인 흐름이라고 보면된다.
  
- Task : 아주작은 작업의 단위이다. 예를들면 마트에서 양파를사오기, 마트에서 계산을하기, 마트에서 장바구니를 구입하기 이런것들 하나하나가 각각의 Task가된다. 그리고 이 Task가 모여서 마트에서 장을보고 나오기 라는 하나의 DAG가 된다.


#### 에어플로우 구성요소

- Airflow_user: 나 (컴퓨터가밥먹길원하는사람)

- 웹서버 : 컴퓨터가 user가시킨 식재료를 제대로 사고있는지, 조리법대로 요리하고있는지 등을 감시하고 관리해서 내게 정보를 제공하는 화면. 내가 직접 컴퓨터에게 명령을 내려도 되지만, 웹 서버에게 명령을 내려도된다.

- 스케줄러 : 워크플로우 시간 관리 담당 비서같은녀석

- 메타스토어 : 여러 DAG들의 정보를 기록해놓은 파일함같은것. 해당 DAG가 왜 실패했는지, 어디서 문제가생겼는지 어떤 오류를 반환했는지를 기록한다.

- 익스큐터 : 작업을 배치하는 작업반장 역할을하는 친구이다. Airflow의 환경설정에서 바꿔줄수있다.

- 워커 : 작업반장에게서 작업을 할당받아서 일을해주는 친구. 

- 오퍼레이터 : 특정 작업을 수행할때 사용되는 기계같은것이다. 식기세척기를 떠올리면 적합하다. 이 오퍼레이터들을 이용하면 하나의 task가될수있다. (DAG:설거지, TASK:접시를닦기, 숟가락을 닦기, Operator:식기세척기) 이런식으로 이해하면 편하다.

<mark>예시</mark>

    with DAG :

        TASK_1:(
            식기세척기 오퍼레이터:
                숟가락 닦기
                ..
                .
        )

        TASK_2:(
            건조기 오퍼레이터:
                접시말리기
                ..
                .
        )

    TASK_1 >> TASK_2  # 이 친구는 DAG의 방향, 즉 파이프라인의 흐름을 지정하는것. 이것까지 완성해줘야 진정한 하나의 DAG가 완성된다.

DAG의 종류로는 

-generic_transfer : sql를 다룰때 주로 사용한다. airflow에서 수집, 가공되고있는 데이터를 지정한 db에 작성한 sql문을 실행하고 적재한다.

-Dummy : 아무작업도 하지않는 연산자로 다른 작업을 그룹화하는데 사용되는 오퍼레이터.

-＊Sensor : Task를 언제 실행시킬 트리거를 기다리는 특별한 오퍼레이터. (어떤 폴더에 데이터가 쌓이고있는지, hdfs에 내가 찾는 폴더나 파일이있는지 등을 감지하는 작업을하는 오퍼레이터)

-PythonOperator : 파이썬 코드, 함수등을 작업하게 도와주는 오퍼레이터.

-BashOperator : bash 명령어를 사용할수있도록 도와주는 오퍼레이터.

-sqlOperator :  sql 관련 작업들을 도와주는 오퍼레이터이다. 

-SimpleHttpOperator : HTTP요청을 보내고 응답 텍스트를 받는 작업을한다.

-HttpSensor : 응답(response)하는지 확인할 때 사용하는 센서 기계

간단하게 위와같은 오퍼레이터 및 센서들만 알아도 어느정도 작업은 다 처리할수있다. 


[Operator_정리](https://yhjin.tistory.com/26)

### Connections

ssh, sql 연동 정보등을 생성 및 등록하여 고유한 id로 연동정보를 불러서 사용한다.

웹서버에서 Admin->Connections 를 통해 등록할수있다.

자세한건 해당 페이지의 Airflow.md를 참조하자.


## Hadoop

하둡을 한줄로 요약하고 설명하자면 **일반수준의 PC 여러대를 하나의 가상 스토리지로 만들어서 스토리지안에 저장된 빅데이터를 병렬처리해줄수있게하는 오픈소스 프레임워크이다** 

그러나 이렇게 말한다면 조금 하둡이라는 녀석을 알기어렵다.

그렇다면 하둡이 어떻게쓰이는지, 왜 하둡이 탄생하였는지를 보면 이녀석의 역할을 잘 알수있을것이다.

#### 하둡의 탄생배경

하둡의 등장은 시대가변하면서 데이터의 타입과 크기가 다양해지고 커지면서 하둡이 등장하게되었다. <u>일반 디스크같은경우는</u> 데이터를 저장할수있는 용량은 크지만 <u>읽는속도는 초당 100MB</u>로, 만약 하드디스크에 저장된 <u>1TB짜리 데이터를 읽는다고친다면 약 두시간이걸릴것이다.</u>

그렇다면 기업에서는 하루에 많은일을 처리하지못하게될것이다. 그러한 문제를 해결하기위해 나온것이 하둡이다.

만약 1TB 크기의 데이터를 100개로 쪼개서 100개의 디스크에 각 100분의 1만큼씩의 데이터를 저장했다고 가정한다면, 크기가 작은 데이터를 불러오는것이기때문에 2분이면 충분히 데이터를 읽어올수있을것이다.

그렇게 데이터를 쪼개서 하둡 클러스터에 저장하는 파일시스템을 HDFS라고하고, 쪼개진 데이터를 계산이나 조작하는것을 도와주는친구가 맵리듀스라고 생각하면된다. 

즉, 하둡은 여러대의 저장소를 하나로 묶어, 하나의 저장소처럼 사용하며 데이터의 분산 저장 및 처리를 도와주는 오픈소스 프레임워크라고 생각하면된다. 

기본적으로 구조는 

HDFS : 하둡파일 시스템.

맵리듀스 : 일괄 질의 처리기. 일괄로 처리하기때문에 짧은 시간내에 응답을 돌려받진못한다. HDFS에 저장된 큰 데이터를 계산하고 처리하는 역할을한다.JSON포맷으로 데이터를 처리한다. (키-밸류) 여튼 핵심은 계산이 맵과 리듀스로 분리되어있고 그 둘 사이를 혼합해주는 인터페이스가 존재한다는것.

YARN : 하둡 클러스터의 자원 관리시스템이다. YARN이 있으므로 맵리듀스뿐만아니라 **어떤분산처리 프로그램도 하둡 클러스터에 저장된 데이터를 처리할수있게해준다.**


으로 이루어져있다.


이러한 하둡은 **하둡에코시스템** 이라고해서 하둡과 함께사용하는 타 프레임워크들과 생태계를 이루어서 살아가고있다고 보면된다.

(하둡이 코끼리이고 하둡을 사용하는 사람들이 자신의 프레임워크의 이미지를 꿀벌, 돼지 등으로 해서 동물원을 연상시킨다고 하둡에코시스템이라고 부른다. 그리고 주키퍼는 말그대로 이것들의 사육사, 즉 관리자역할을하는 프레임워크가되는것이다.)

하둡은 타 프레임 워크들과 함께 작동할때 더욱 좋지만, 우리는 일단 하둡이라는녀석을 알아봐야하기때문에 위에서 언급한 하둡의 기본구성 **(HDFS, 맵리듀스 , YARN)** 을 알아볼것이다.

#### HDFS

위에서도 언급했듯이 하둡은 아주 큰 빅데이터를 하둡 클러스터에 분산저장하여 관리하고 잘 정리정돈하는 친구이다.

그렇기때문에 하둡에 파일을 저장하면 **하둡은 블록단위로 데이터를 저장하게되고**, 이 블록은 한개당 128MB 크기를 가진다. 왜 128MB이냐면 우리는 어떤 크기의 비정형,정형 데이터를 저장할지 모르고 가장 계산하기 쉬운 단위가 128MB이기때문에 기본적으로 128MB로 설정되어있는것이다.

블록의 크기는 **hdfs-site.xml의 dfs.blocksize 속성**을 사용해서 바이트 단위로 설정해줄수있다. (256,268,435,465등등.. 네임노드의 메모리부담을 줄이면서 매퍼가 더많은 데이터를 처리할수있도록하는것이다.)

#### NameNode

하둡은 분산처리시스템이기때문에 '어떤곳에 어떻게 저장하고 관리할것인가'를 담당하는 관리자가 꼭 필요하다.

그러한 역할을하는것을 보통 MasterNode라고 표현하고 HDFS에서는 NameNode, 맵리듀스에서는 Job Tracker 라고 부른다. (Task Tracker는 DataNode와 같은개념 다만 맵리듀스에서 사용하는 단어이다.)

즉, **NameNode**는 해당서버에 워커로 등록된 DataNode들을 관리하는 *마스터*인것이다.

NameNode는 DataNode에게 저장해야할 데이터를 분배하는 역할을하는것이다.

얼만큼의 크기의 블록으로 들어오는 데이터를 쪼갤것인지, 들어오는 데이터에대해서 복제본을 몇개를 생성할지 이러한것들을 정해서 DataNode에게 저장하라는 명령을 내리는것이다.

NameNode는 마스터 역할을하기때문에 관리자가있는 서버에 장애가생겨버리면 실제 데이터를 저장하고있는 DataNode을 사용할수없기때문에 **부하가 가장 적은 환경으로 구축**한다.

그러니, 아주 당연하게도 NameNode는 하둡클러스터 내부에있지만 하나의 Node에서 단독으로 NameNode를 실행하게된다. NameNode를 실행시키는 노드에서는 NameNode만 실행시키고 DataNode들과 같이 실행시키지않는다.

또한, NameNode에는 HDFS에 업로드한 파일이나 데이터를 실제로 저장하지않는다. (당연히 부하가 적은 환경에서 실행되고있어야하니까 데이터의 저장을 하지않는다.)

<u>NameNode의 역할 간단하게 정리</u>

- 메타 데이터 관리 : 파일 시스템을 유지하기 위한 메타데이터를 관리함
- 데이터노드 모니터링 : 3초마다 Hart-beat를 전송한다(워커가 동작중인지 주기적으로 확인해야하니까) DataNode도 NameNode에게 Hart-beat를 전송함으로 서로간에 주고받는다. 10분이상 반응이없을시 NameNode는 DataNode가 죽었다고 판단한다.
- 블록관리 : 장애가 발생한 데이터노드가있을수있으니 복제. 어떤 파일이 어떤 블록으로 어떤 데이터노드에 저장되어있는지 기록한 장부같은것도 가지고있음. 그걸보고 파일을 반환함
- 파일시스템에 이미지파일 관리(fsimage)
- 파일시스템에 대한 Edit Log관리

※메타데이터란?

전체적인 구조를 뜻함. 파일시스템이 어떤 구조로 이루어져있는지에 대한 전체적인 관리를 NameNode가 한다.

※fsimages란?

파일시스템 이미지 관리를 fsimages 라는 이름으로 NameNode가 떠있는 디스크에 저장을하게되어있다. 
이 파일은 매우매우 중요한데, 해당 이미지파일에 손상이 생길경우 하둡안의 데이터가 다 손상되는 상황이 발생할수있다.
일종의 **스냅샷**같은것.

※Edit Log

fsimages가 스냅샷같은것이라고 설명했으니, 저 스냅샷이후로 변경된 사항들이나 기타 사항들에대한것은 Edit Log에 작성된다.


#### DataNode

NameNode가 마스터노드였다면 DataNode는 워커노드이다. 관리는하지않고 NameNode에게서 전달받은 데이터를 지정된 블록단위만큼 쪼개서 저장하고 지정된 복제본의 갯수만큼 백업용으로 복제해서 가지고있는다.

여기서 복제본 같은경우는 같은 DataNode에 저장되지않는다. 다른 DataNode과 복제본을 나눠갖게되는 구조이다.

지정된 블럭의 단위만큼 파일을 쪼개서 관리하되, 하나의 원본을 저장하되 지정한 갯수만큼의 복제본을 같은 rack에도 저장하고 다른 rack에도 저장하는 방식으로 저장이된다.

그렇다면 HDFS에는 한개의 원본 + 지정한 복제본 만큼의 쪼개진 블럭을 나눠가지게되는것이다.


![hdfs의저장방식](./Hadoop/hdfs저장방식.PNG)

▲ 위와 같은 구조가되는것이다.

파일을 쪼개서 A B C 가나왔다고 친다면 RACK1의 데이터노드1번에 A블럭 저장(원본) RACK1의 데이터노드2에 A블럭 저장(사본) RACK2의 데이터노트3번에 A저장(사본2)


그럼 여기서 이런 생각이든다. <u>'엥 굳이 복제본을?'</u>

이유는 간단하다. 여기저기 분산해서 저장해놨는데 하나의 노드가 고장이나버렸다고 가정하자.

만약 이런상황에 복제본을 만들어놓지않았다면 문제가생길것이다.

HDFS에서 복제본을 지정하고 여러 DataNode에 저장하는이유가 바로 이것이다. HDFS는 복제본을 여러곳에 저장함으로써 안정성을 보장한다.

<u>DataNode 역할 간단하게 정리</u>

- 클라이언트가 HDFS에 저장하는 파일을 로컬 디스크에 유지
- 실제 데이터를 저장하는역할
- 알번적으로 레이드 구성을하지않음(패리티 체크 들어가고하면 디스크의 전체 사용용량이 줄어들어버리니까)
- 블록 리포트 : NameNode가 시작될때, 주기적으로 로컬파일시스템에있는 모든 HDFS블록들을 검사 후 정상적인 블록의 목록을 만들어 NameNode에 전송.만약 문제가있는 블록이있는것을 NameNode가 보고받는다면 해당 DataNode에게 그 블록을 지우게하고 같은 블록을 저장하고있는 다른 DataNode에게 똑같은 블록을 장애가생겼던 DataNode에게 복사하도록 명령을보낸다.

여기서 우리가 주의해야할점은 HDFS는 위에서 말한것과같이 지정된 블록단위로 쪼개서 저장하기때문에, 쪼개고 남은 용량은 지정된 블록만큼 용량을 차지하진않고 지정된 블록의크기내에서 쪼개고 남는만큼의 용량만 차지한다.


EX) 612MB 짜리 원본파일 | 블록단위 128MB 라고 치면, 128/128/128/128/100 의 용량을 가진 블록들로 나눠서 저장된다.


보통 하둡 클러스터를 구성할때 DataNode디스크는 Raid를 사용하지않는다.

(왜냐면 DataNode는 HDFS를 통해서 블록으로 쪼개진 블록파일들이 로컬디스크에 '실제로 저장'되기때문에 Raid를 하면 사용할수있는 전체 디스크 용량이 줄어드니까.)

#### 블록에대하여

하둡 2.X 부터는 블록의 기본크기가 128MB로 커졌다. 왜 블록의 단위가 커진걸까?

그건 **탐색**과 관련이있다. 탐색에대해서는 아래 하둡단어쪽에 잘 정리를해놨으나, 간단하게 말하자면 디스크에서 데이터를 찾기위해 헤더를 특정 위치로 옮기는 작업. 정도이다. 탐색의 코스트가 클수록 디스크에서 데이터를 찾아오는 속도가 느려지게된다.

그래서, 블록이 커진이유는 바로 이 **탐색 비용을 최소화하기위해**이다.

블록이 크면 하드디스크에서 블록의 시작점을 탐색하는데 걸리는 시간을 줄일수있고, 이렇게 절약한시간을 네트워크를 통해 데이터를 전송하는데 더 많은시간을 할당할수있다.


HDFS는 기본적으로 블록의 복사본을 3개를 만들도록 설정되어있다.
만약 사용자가 하둡을 설치할때 conf파일에 기본적으로 복사본을 만들 갯수를 조정해놨다면 조정한 갯수만큼의 복사본을 만든다.

이러한 복사본은 위에서 말했던것처럼 매우 중요한 역할을한다. 장애가 났을시에도 유용하지만, 결과적으로 NameNode는 DataNode에 장애가생겨서 지정해준 값만큼의 복사본을 유지할수없게되면 장애가 난 DataNode가 가지고있는 블록들의 복사본을 가지고있는 다른 DataNoode들에서 백업해놨던 복제본 블록을 복제해서 또 다른 DataNode에 복제하여 <u>블록 복제본의 갯수를 항상 일정하게 유지하게된다.</u>

그럼 HDFS에 파일을 저장하게된다면 NameNode가 DataNode에게 지정한 갯수만큼의 복사본을 나눠저장하게하라고 명령을내리고 장애가 생겼을시는 복사본의 갯수를 유지시키기위해 DataNode에게 또 다른곳에 복사하라는 명령을 내리며 관리가된다면, 아주 당연하게도 하나의 파일을 HDFS에 저장하게되면 원본파일크기*지정한복사본갯수 만큼의 용량이 필요하다는것을 알수있다.

기본 복사본 3개로 설정되어있다 가정하고 24GB짜리를 HDFS에 저장한다면 24*3 = 72GB, 72GB로 저장되는것이다.

##### 세컨더리네임노드

NameNode를 실행시키면 처음에 해당 디스크 어딘가에 저장되어있는 fsimages를 읽어서 (스냅샷같은기능) 메모리에다가 그 스냅샷을 구성을한다. 
이후 Edit Log를 쭉 읽어서 메모리에 반영을시킨다.
그러한 모든 작업을 마치면 NameNode는 실행이된다.

세컨더리 네임노드를 설명하는데 갑자기 NameNode의 설명? 이라고 생각할수도있지만, 사실 세컨더리 네임노드에 장애가 생긴다고해도 하둡 실행에는 큰 문제가없다.

다만, 위에서 말한것처럼 Edit Log 파일의 크기가 무한히 커진다는 문제점이 생긴다. (왜냐하면 계속 우리는 시스템을사용하니까 변경점이나 기타사항이 계속 log에 기록되니까.)

<u> 그럼 이 세컨더리 네임노드는 무슨 역할을 하는가?</u>

시스템에 변경점이생겼을때 Edit Log에 기록이남으니 NameNode의 fsimages와 Edit Log는 주기적으로 병합해줘서 관리를해줘야한다.
(아니면 log파일이 계속 커지니까)

하지만 병합하는 작업은 NameNode에서 실행시키지않고 세컨더리 네임노드에서 실행시킨다.

해당 데몬은 NameNode에서 Edit Log와 fsimages파일을 세컨더리 네임노드로 보내고, <u>fsimages에 Edit log 를 Merge하는 작업을 세컨더리 네임노드에서 처리하고 합쳐진 fsimages파일을 NameNode에있는 기존 fsimages파일과 바꿔치기하는 작업을 주기적으로 실행</u>하는 역할을하는 고마운 친구이다.

※fsimages파일을 저장하는 경로의값을 복수로 늘려서 여러군데에다 동시에 저장하게하는 방법도있다. 주로 2.x을쓰는 기업에서는 이런방식으로도 fsimages파일을 관리한다고 한다.

#### 각 버전별 HDFS특성

- 하둡 1.X : 블록기본크기 64MB. NameNode에 장애가 발생하였을떄 대비책이없다.
- 하둡 2.X : NameNode에 장애가 발생하였을때의 대비책이있다.
- 하둡 3.X : 기존파일의 2배의 용량으로 HDFS를 운영할수있는 기술도입. 이것을 **Erasure Coding**이라고하는데, 이것은 하둡 내부적으로 Raid를 구성하는것과 같다고 생각하면된다.

※Erasure Coding?

보통 2.x대에서는 100mb의 파일을저장하기위해 레플리카포함 300mb 내외의 저장공간이 필요했는데, 약 200mb 정도로 줄여줄수있는 알고리즘 방식이다. 디렉터리나 파일단위로 Erasuer Coding을 적용하고, 적용하지않고, 레플리카를 그대로 유지할수도있다.

주로 적용하지않는 파일들이나 디렉토리는 용량이 매우작은경우이고

적용하는 파일이나 디렉토리는 용량이 큰 경우이다.

  
### 하둡 유용한 커맨드

```bash
hdfs dfsadmin -report :hdfs의 각 노드들의 상태를 출력하며 hdfs의 전체 사용량과 각 노드의 상태를 확인할수있음.

hdfs dfsadmin -report -live : live한 data node포함 hadoop상태 레포트
hdfs dfsadmin -report -dead : 죽은 data node 레포트
hdfs fsck [하둡파일경로]-files -blocks -locations :저장한 파일이 어디어디에 분산저장되었는지 확인

hdfs dfs balancer : 서로다른 스펙의 데이터노드를 하나의 클러스터로 구성하게될때 노드간 디스크 크기가 다르거나 등의 문제로 전체 데이터의 밸런싱이 되지않는 문제가 생겼을때 해결하는 커맨드. (신규 데이터 노드를 추가하는 경우에도 발생할수가있다.)

#하둡파일에서 balancer설정 
#hdfs-site.xml

<property>
    <name>dfs.datanode.balance.max.concurrent.moves</name>
    <value>50</value>
</property>

<property>
    <name>dfs.datanode.balance.bandwidthPerSec</name>
    <value>104857600</value>    
</property>


```

#### 맵리듀스

이 친구는 HDFS에 저장된 데이터를 계산 및 처리하는 역할을한다.
하지만 아까 위에서도 말했듯이 일괄 질의 처리기고 이녀석은 RDBMS와 좀 다르다. RDBMS는 상대적으로 크기가 작고  **정형데이터** 를 처리하는데 특화되어있다는 강점을 가지고있다. 그러므로 특정 쿼리와 데이터를 변경하는데 적합하다 (상대적으로 크기가작은)

또한 분산처리되어있는 데이터를 다시한곳으로모아 통합처리를한다면, 데이터를 계산하거나 처리하는데 매우 많은 시간이 들고 비효율적이게된다.

그래서 맵리듀스는 각각의 노드에서 처리를한 결과를 통합하는것이다.

각각의 처리는 맵이하고 리듀스는 통합하여 결과를 반환한다. 그리고 그 리듀스의 출력값을 중앙에 넘겨주는 흐름이다.

하지만 시대는 '빅데이터'라는 단어가 생겨날만큼 데이터의 크기가 커졌다.
수집되는 데이터는 정형데이터가아니라 이미지, 동영상, 텍스트 등 비정형 데이터가 주를 이루면서 수집되고 이동되는 데이터의 크기가 늘어나버린것이다.

빅데이터: 1024TB는 1PB(페타 바이트)가 된다. 일반적인 빅데이터는 현재 페타 바이트 단위이다.

데이터의 단위 : 비트(yes or no) < 1바이트(알파벳1 숫자1) < 1킬로바이트kb(몇개의 문단) < 메가바이트mb(1분길이의 mp3노래) < 1기가바이트 (30분 길이의 HD영화) < 1테라바이트(195~200편의 FHD영화) < 1페타바이트pb 

※ 1024배씩 단위가 커진다. 1바이트는 1024비트, 1기가바이트는 1024메가바이트.

RDBMS와 맵리듀스의 차이를 표로 표현한다면

|구분|RDBMS|맵리듀스|
|:-----:|:-----:|:----:|
|데이터크기|기가바이트|페타바이트|
|접근방식|대화형과 일괄 처리방식|일괄 처리방식|
|변경|여러번 읽고 쓰기|한번 쓰고 여러번 읽기|
|트랜잭션|ACID(원자성,일관성,고립성,지속성)|없음|
|구조|쓰기 기준 스키마|읽기 기준 스키마|
|무결성|높음|낮음|
|확장성|비선형|선형|


맵리듀스는 크게 맵(매퍼) / 리듀스로 단계로 나눠져있다. 

각 단계는 입-출력 으로 나눠져있고 JSON파일형식으로 데이터를 처리한다.

맵 단계에서 입력,처리하여 출력한 값은 리듀스가 입력으로 건네받게되고, 리듀스는 사용자가 지정한 리듀스단계에서 수행해야할 일을하여 최종적으로 결과물을 출력하게된다.

그럼 어떤 단계에 어떤 작업을 맡기는게 좋을까?

맵 단계에서는 주로 **이상치** (결측값, 문제가있는 값이나 레코드 등)를 걸러주는 작업을 수행하는것이 적합하다.
이렇게 걸러진 JSON값들을 리듀스의 입력값으로 넘겨서 리듀스는 깨끗한 데이터세트에서 우리가원하는 값을 찾아 출력하게된다.

단, 맵 -> 리듀스 단계로 넘어가는 과정에서 JSON값들은 KEY를 기준으로 정렬되고 그룹화하게된다.

    EX) 

    #맵 출력
    

    (1950, 11)
    (1950, 12)
    (1950, 03)
    (1821, 01)
    (1822, 07)
    (1821, 05)

    #리듀스 입력
    #reduce (key, list(value))

    (1821, [01,05])
    (1822, [07])
    (1950, [11,12,03])



맵에서 리듀스로 넘어가는 단계에서 위와같은 작업이 이루어진다. 아까 말했던것처럼 KEY를 기준으로 정렬되고 그룹화하게되는것이다.

그렇다면 내가 만약 리듀스 단계에서 각 년도가 가지고있는 가장 마지막달을 구하는 함수를 작성하였다면, 리듀스 단계에서는 위와같이 입력받고 아래와같이 출력하게된다. 
    

    (1821, 05)
    (1822, 07)
    (1950, 12)


이제 맵리듀스가 뭘하는 친구인지 간단하게 알아보았다. 맵단계에서 입력할때, 어떤 데이터를 어떤 타입으로 넣을지는 사용자가 정할수있다.
각 행의 타입을 텍스트로 인식하는 텍스트 입력 포맷을 선택하면 텍스트이면서 키-값 의 형태를 가지게된다.

#### YARN

YARN은 하둡2.0부터생긴 **하둡클러스터 리소스 매니저**이다.
맵리듀스같이 분산처리 알고리즘, 시스템을 YARN 이라는 녀석위에서 작동시킬수있고, 맵리듀스뿐만아니라 **다른 분산처리 시스템**도 YARN에서 실행시킬수있다.

이말은 즉, YARN라는 친구는 하둡안에 원래 내장이되어있던 맵리듀스가 하둡 클러스터내부에있는 자원(HDFS파일 등)을 사용할수있게하는 중간 다리 같은 역할을하는것이다.

![YARN의위치](https://www.puzzledata.com/wp-content/uploads/2020/05/2-1-1.png)


이 YARN이라는 녀석은 하둡 2.X부터 나온친구이고, YARN을 사용하게 하둡을 설정하면 데몬에 **리소스매니저**라는 데몬이 하나 더 뜨게된다.

- 리소스 매니저 : 하둡 클러스터내부에 딱 1개. 하둡 클러스터 전체의 리소스(자원)을 관리하는 역할을한다. 클라이언트가 요청한 어플리케이션마다 자원을 관리.

- 노드 매니저 : 각 슬레이브 노드마다 1개씩있다. 컨테이너와 자원의 상태를 RM(리소스 매니저)에게 통지한다.

- 어플리케이션 마스터 : 각 어플리케이션마다 1개.어플리케이션의 실행을 관리하고 상태를 RM에 통지. 중요한건 <mark>어플리케이션 마스터 실행 요청이들어와야 실행된다. 평소에는 실행x </mark>

- 컨테이너 : 어플리케이션을 수행할수있는 역할을한다. 제한된 자원을 소유하고 제한된 자원만큼 컨테이너를 만들어 그 안에서 어플리케이션을 수행한다. 상태를 리소스매니저에게 통지.
  

그러니 이 YARN이라는 친구가 생김으로써 실행되는 데몬은 위와 같이 하둡 클러스터 내부에 하나있는 리소스매니저, 각 노드마다 하나씩있는 노드 매니저, 평소에는 동작하지않고 가만이있다가 RM에게서 작업 요청이들어오면 실행되는 어플리케이션 마스터 로 구성되어있다.


그러니까 유저가 어떤 어플리케이션을 만들어서 일을 제출하게되면(YARN에게 서브밋하게되면), <u>리소스 매니저가 전체 클러스터의 리소스 상태를 주기적을 보고 받고있다가 리소스 매니저가 어플리케이션 마스터(각 어플리케이션마다 한개있는것)을 어느 노드에서 실행시킬지 선출해서 선출된 노드매니저에게 명령을 내리게된다.</u>


그렇게 리소스 매니저에게 명령받은 해당 노드매니저의 노드는, 평소에 존재하지않고 가만이있던 어플리케이션 마스터를 (1.x버전으로친다면 잡트래커임, 일관리자) 구동시켜서 일을 처리한다.

YARN의 동작구조를 보면 아래와 같다.

![YARN아키텍쳐](https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/03/apache-hadoop-yarn.jpg)

하둡2.X버전부터는 잡트래커나 태스크 트래커가 없어지고 YARN이 생기면서 위의 그림같이 **리소스매니저, 노드매니저, 어플리케이션마스터, 컨테이너** 등 새로운것들이 생겨났다.

또한 YARN같은 경우 위의 그림과 설명대로 1.X버전대보다 실행이 복잡하게 바뀌어서 작은 단위의 작업을 처리하는데 YARN을 이용하는것은 비효율적이다.

왜냐하면 YARN에게 어떠한 작업을 제출하게되면 이 제출된 작업을 실행하기위해서 자바에서 어떤 머신을 포크하고, 기타 등등 여러가지의 작업을 거쳐서 구동시키고 리소스 매니저가 노드매니저들에게 상태를 보고받아 체크하고 일을 시킬 적절한 노드를 선출하고 또 어플리케이션 마스터를 구동하고,  등등의 많은 사전작업들을 하기 때문이다.

그렇기때문에 아주 간단한 작업을 제출한다고해도 20초 ~ 30초가 걸리는것이고 작은 데이터를 처리하기에는 적합하지않은것이다.

YARN을 조금 정리해보자면

-어플리케이션은 메모리의 최소할당과 최대할당에 대한 요청이 가능하다.
-기본적인 메모리 할당은 스케줄러에 지정되어있다.
-YARN에서는 진행상황과 상태정보를 어플리케이션마스터에게 보고
-클라이언트는 진행 상황의 변화를 확인하기위하여 매초마다 어플리케이션 마스터를 조회
-YARN에서 진행상황 모니터링은 리소스매니저의 웹UI를통해 실행중인 모든 어플리케이션을 확인할수있고, 각 링크가있는데 이것이 <makr>어플리케이션의 마스터의 웹UI로 연결된다.</mark>

어플리케이션 마스터의 웹 ui로 연결된다는 말은 곧 리소스매니저가 어느 노드의 노드매니저에게 일을 시킬지 우리는 모르므로 언제나 바뀔수있다.


#### 하둡의 주요 단어

-노드 : 하나의 컴퓨터. 하나의 서버. 즉 클러스터로 묶을수있는 구성원. 30~4-개의 노드가모여 하나의 rack을 이룬다.

-rack : 노드들이 모여 하나의 rack을 이룬다. rack은 물리적으로 같은 네트워크의 스위치에 모두 연결되어있다. 데이터의 이동할수있는 폭이 크기때문에 속도가 빠르다.

-하둡클러스터 : 여러개의 rack이 모여 하나의 하둡 클러스터를 구축하게된다.

-탐색 : 디스크상에서 탐색이라는것은 데이터를 읽거나 쓸 때 디스크의 헤더를 디스크의 특정위치로 이동시키는 조작이다. 디스크의 전송속도는 디스크의 대역폭과 관계가있다. 대역폭이 작을수록 데이터를 읽어오거나 작성하는데 시간이 줄어든다.

-B트리 : 관계형 데이터베이스에서 사용되는 자료구조이다. 탐색을 수행하는 속도에 제한이있다. 데이터베이스의 상당 부분을 변경할때 B-트리는 데이터베이스를 재구성하기위해 소트와 머지를 사용해야하므로 맵리듀스보다 비효율적이다. 이것이 빅데이터를 RDB에서 처리하는것이 비효율적인 이유이다.


-★블록의 지역성(Locality) : 맵리듀스같이 하둡내에서 어떤 일을 시킬때 작업에 필요한 정보를 가지고있는 블록만 일을하도록 NameNode나 JobTracker가 일을시킨다. 또한 해당 블록을 가지고있는 DataNode는 **네트워크를 통해 데이터를 전송하지않고** 일단, 로컬에있는 블록데이터를 가지고 연산을 실행한다. 이것을 블록의 지역성이라고부른다.

    ex) 기온정보를 담고있는 파일의 블록이 3개로 쪼개져있고 각각 DataNode 1,5,12번에 저장되어있다고 가정한다면, 맵리듀스를 통해 년도별 기온정보를 추출하는 코드를 작성했다면 기온정보 파일의 블록을 가지고있는 1,5,12번 DataNode만 로컬에 가지고있는 블록정보로 작업을 시작하는것.

## Spark

하둡에 저장된 자원,데이터들을 분석하고 조작하는 일종의 처리기다.
하이브랑 유사하게 하둡내부에있는 데이터를 sql로 배치로 사용할수있게 해주고 하이브에비해 속도도 굉장히 빠르다. 
spark가 빠른이유는 **메모리상에서 데이터를 처리하기때문이다.**


그럼 Spark의 구조는 어떨까?

크게 RDD와 DataFrame 으로 구분되는데 리스트형식으로 RDD를 구성하고 RDD를 DF로 만들기위해 차원으로 구성하면 DF가 만들어진다.


다만, 단점이있는데 메모리상에서 데이터를 처리하기때문에 <u>전체 클러스터의 메모리 사이즈에서 벗어날만큼 큰 데이터의 처리는 할수없다는것이다.</u>  보통은 메모리보다 디스크가 더 크기때문에, 스파크에서 처리하지못한 데이터는 하이브에서 처리할수있다.
<u>(하이브는 디스크단위로 읽고쓰기때문에)</u>
 

## Zookeeper

주키퍼는 하둡 에코시스템을 관리하거나 분산시스템을 관리하는 **분산코디네이터**라고 보면된다.

주키퍼 서비스는 복수의 서버에 복제되며, 모든 서버는 데이터 카피본을 저장한다.

주키퍼는 보통 n개의 서버로 단일 클러스터를 구성하고 이렇게 구성한것을 서버 앙상블이라고하는데 주로 **홀수**단위로 구성을한다.

zookeeper를 설치하고 실행시켜보면 먼저살아나는 순서대로 리더와 팔로워가 나누어진다. 언제나 리더와 팔로워가 바뀔수있는것이다.

팔로워 서버들은 클라이언트로부터 받은 모든 업데이트 이벤트를 리더에게 전달한다. 또한 클라이언트는 모든 주키퍼 서버에서 읽을수있으며, 리더를 통해 쓸수있고 과반수 서버의 승인이 필요하다.

여기서 중요한건 <u>과반수</u>이다.이 주키퍼를 홀수로 구성하는이유는 주키퍼는 살아있는 서버가 과반수이면 계속 서비스를 유지할수있는 특성을 가졌기때문이다.


주키퍼는 분산 베타적 잠금이라고해서 이것은 DB의 락 시스템을 생각하면 어렵지않을것이다.
서로다른 어플리케이션이 데이터를 쓰거나 변경하려고하면, 먼저 쓰거나 변경을하고있는 어플리케이션의 작업이 끝날때까지 아무도 데이터를 읽거나 쓸수없는것이다.


주키퍼 내부에는 **z노드라는** 녀석이있다.

그럼 이 <u>z노드는</u> 무엇인가?

z node란 Zookeeper에서 데이터가 저장되는 단위로서, 데이터가 저장되는트리의 노드라고 생각할 수 있겠다.

총 세종류의 znode로 분류하는데 영속의 종류에따라 세 종류의 차이점이 생긴다.

1. Persistent Nodes(영구노드) : 명시적으로 삭제되기 전 까지 존재한다.
2. Ephemeral Nodes(임시 노드) : 세션이 유지되는동안 활성(세션이 종료되면 삭제된다.) 자식 노드를 가질수없다.
3. Sequenece Nodes(순차노드) : 경로의 끝에일정하게 증가하는 카운터가 추가된것. 영구 및 임시노드 모두에적용가능하다.
   

![트리형태의znode](https://www.oreilly.com/api/v2/epubs/9781784391324/files/graphics/1324OS_02_02.jpg)

위와같은 형태를 가진것이 znode인데 간단하게 설명하자면 

- '/' 처럼 절대경로로 구분된다.
- 변경이 발생하면 버전 번호가 증가한다.
- 데이터는 항상 전체를 읽고쓴다
- znode는 1m이하의 데이터를 가질수 있으며 자식노드를 가질수있음

ZooKeeper는 여러 서버에 분산되어 있는 znode를 관리하기 위한 서비스이다. znode는 메모리에 저장되어 빠른 속도를 보장하지만, 크기에 제한을 갖는다. 그러므로, ZooKeeper는 여러 클러스터에 공유되어야 하는 설정 값이나 리소스 상태 정보등을 저장할 때 매우 유용하다.

znode에 분산관리가 필요한 서비스같은것들을 node형식으로 znode에 붙여서 그것을 주키퍼가 관리할수있도록 만드는것이 주키퍼의 역할이다.

znode에서 node를 삭제시키고 주키퍼에서 관리하길원하지않는다면 node를 삭제시킴으로써 주키퍼에서 탈퇴를할수있다. 반대로 등록은 가입이되는것이다.

## 단어

-데이터 마이닝 : 결과를 예측하기위해 대용량 데이터세트에서 의미있는 패턴이나 통계적 규칙을 찾아내는것.

-데이터 파이프 라인

-ETL

-UDP

-TCP

-정형 데이터 : 날짜, 시간 및 고객 ID같이 미리 정의된 특정 스키마를 가진 데이터베이스 테이블과 같이 형식이 정해진 항목으로 구조화되어있음.값과 숫자로 구성되어있다. 의미파악이 쉽고, 규칙적으로 입력되는값을 정형 데이터라고 볼수있다. (데이터베이스에 들어가있는 값이라고 다 정형은아니다)

-반정형 데이터 : 정형데이터에 비해 스키마가 유연하거나 생략되있을수있다. 데이터 구조에대한 최소한의 규칙만있으면된다. 그리드 형태의 셀 구조로된 스프레드시트는 셀 자체에 어떤 형태의 데이터도 담을수있는것처럼말이다. 비정형과 반정형이 섞인것이다. JSON 형식을 예로 들어본다면 json형식의 글 내용본문은 반정형이다(비+정 섞임), db에 저장된 글제목 > 비정형, db에 저장된 성별 > 정형

-비정형 데이터 : 어떠한 내부 구조가없는것. 일반 텍스트나 음성 이미지 영상 등 아무런 규칙도 구조도 없는 데이터들을 의미한다.

-선형

-비선형

-객체

-타임시리즈 데이터 : 시간의 흐름에따라 계속해서 생성되는 데이터.

-온프레미스 : 기업의 서버를 자체적으로 보유하고있는 서버에 직접 설치하고 운영하는 방식. 클라우드 이전에 가장 일반적으로 사용하던 시스템이고 직접인프라를 구축하는 방식을 의미한다.






